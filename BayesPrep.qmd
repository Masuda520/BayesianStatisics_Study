---
title: "ベイズ統計学事前準備"
format: 
 html:
  theme: default
  toc: true
  toc-title: 目次
  number-sections: false
  self-contained: true
  coe-tools: true
  code-fold: true
  # embed-resources: true
  # self-contained-math: true
execute: 
 error: false
 warning: false
author: Riku Masuda
---

## この文書について  
ベイズ統計学の本を読む際に、事前に知っておいた方が良いベイズ統計の基本的な考え方・ノーテーションについて簡単にまとめたものです。  

## 1. 同時確率分布・周辺確率分布・条件付き確率分布    
いきなりベイズ統計学の話に入る前に、確率分布に関わる基本用語をおさらいします。  

### 同時確率分布  
二つの確率変数$y,\theta$を考えます。  
$y$が確率変数なら、小文字ではなく大文字の$Y$で表すべきでは？　と思うかもしれませんが、**ベイズ統計学では確率変数なのか実測値なのかについて、大文字・小文字での区別をつけないことが多い**ため、この文書でもあまり区別しないことにします。  
$y$と$\theta$は互いに相関しているとします。具体的には、相関係数$\rho = 0.9$であるような2次元の多変量正規分布にしたがっているとします。ただし、$y,\theta$は両方とも、平均と分散がそれぞれ0,1だとします。  
これを数式で表すと以下のようになりました。  
$$
\begin{pmatrix}
   y \\
   \theta
\end{pmatrix} \sim 
N_2 \left(\begin{pmatrix}
   0 \\
   0
\end{pmatrix} , \begin{pmatrix}
   1 \ \ 0.9 \\
   0.9 \ \ 1
\end{pmatrix} \right) 
$$

Rを使って乱数$(y,\theta)$を500個生成して、散布図を描いてみます。  
```{r}
pacman::p_load(tidyverse,mvtnorm,ggExtra)
mu = c(0,0) ; Sigma = matrix(c(1,0.9,0.9,1),nrow=2,ncol=2 )
set.seed(10)
data = rmvnorm(n = 500 , mean = mu , sigma = Sigma ) %>%
  tibble(y = .[,1] , theta = .[,2])
data %>%
  ggplot(aes(x = theta ,  y = y) ) + 
  geom_point() + 
  theme_minimal()
```

横軸$\theta$と縦軸$y$には正の相関関係があるとわかります。  
複数の確率変数が同時に生成されるとき、それが従う確率分布のことを、同時確率分布と呼んだのでした。  
二つの確率変数$y,\theta$の同時確率分布の確率密度関数を、$p(y,\theta)$と書くことにします。  
多変量正規分布の密度の値は、mvtnormパッケージのdmvnorm()関数を用いれば計算できます。  
```{r}
#| code-fold: false
## y=0,theta=0の密度
dmvnorm(c(0,0) , mean = mu , sigma = Sigma)
## y=2,theta=2の密度
dmvnorm(c(2,2) , mean = mu , sigma = Sigma)
```

これを式で表すと、
$$
p(y=0,\ \theta=0) =  0.3651265,\ \ \  p(y=2,\ \theta=2)=0.04447738
$$

となります。  
なお、$y,\theta$は正の相関を持つので、$y=2,\theta=-2$のような組み合わせはとても珍しいものとなり、確率密度$p(y=2,\theta=-2)$は小さくなります。  

```{r}
#| code-fold: false
## y=2,theta=-2の密度
dmvnorm(c(2,-2) , mean = mu , sigma = Sigma)
```

ベイズ統計学では、確率分布のことをその確率密度関数で表現してしまうことが多いです。  
つまり、
$$
\begin{pmatrix}
   y \\
   \theta
\end{pmatrix} \sim p(y,\theta)
$$

と書いてしまうことが多いです。  

### 周辺確率分布  
さきほど、確率変数$y,\theta$は同時確率分布$p(y,\theta)$にしたがっていました。  
このうち、$\theta$を無視して、$y$の分布だけに着目してみます。  
```{r}
data %>%
  ggplot(aes(x = theta ,  y = y) ) + 
  geom_point() + 
  xlim(-3.5,4.5) +
  geom_line(tibble(x_vec = 3+3*dnorm(seq(-4,4,0.01)) , y_vec = seq(-4,4,0.01)), 
            mapping = aes(x = x_vec, y = y_vec), size = 1 , col = "red" ) + 
  theme_minimal()
```

```{r}
data %>%
  ggplot(aes(x = y)) + 
  geom_histogram(aes(y = after_stat(density)), fill = "blue") + 
  stat_function(fun = dnorm, col = "red", size = 1) + 
  labs(title = "yのヒストグラムと密度関数", x = "y", y = "密度") + 
  theme_minimal()
```

$y$の従う分布は平均0分散1の正規分布$N(0,1)$となります。  
$y$が従う分布を周辺分布（周辺確率分布）と呼びます。同様に、$y$ を無視して$\theta$が従う分布を考えた時も、これのことを周辺分布と呼びます。**周辺分布とは**、複数の確率変数の中で、**特定の変数にだけ注目したときの確率分布**のことを言うのでした。  
$y$の周辺分布に対応する密度関数のことを、$p(y)$と書きます。$p(y)$のことを、周辺確率密度関数と呼びます。  
この書き方では、同時密度関数$p(y,\theta)$と同じく、$p$を関数の名前として使ってしまっているので、周辺密度関数と同時密度関数の区別がつかなくて問題があると思うかもしれません。実際その通りで、**単に$p(\cdot)$と書いただけでは、これが何の確率密度関数を表しているのかは、文脈からしかわかりません**。ベイズ統計学ではあまりに確率密度関数を書く機会が多いので、**区別がつかなくなるリスクを承知の上で、単に$p(\cdot)$とだけ書いて、確率密度関数を表現することが多い**です。  

周辺確率密度関数は、数学的な定義では、同時確率密度関数を積分することで得られるのでした。  
$$
\begin{equation*}
\begin{split}
p(y) &= \int p(y,\theta) d\theta \\
p(\theta) &= \int p(y,\theta) dy
\end{split}
\end{equation*}
$$

なお、先ほどと同様に、確率変数$y$が従う分布（周辺分布）を単に$p(y)$と書いて、
$$
y \sim p(y)
$$

と表現できます。  

### 条件付き確率分布  
今考えている例では、散布図からもわかるように、「$\theta=2$のとき$y=0$である確率は低い」です。$y$と$\theta$は強く相関するので、$\theta$の値が高いときは、$y$の値も高くなるためです。同様に,「$\theta=-2$のとき$y=0$である確率は低い」です。    
$y$の周辺分布を考えた時は$y=0$である確率はむしろ高いくらいですが、$\theta$の値を"固定"した場合、$y$の従う確率分布は周辺分布とは異なるものとなります。  

```{r}
data %>%
  ggplot(aes(x = theta ,  y = y) ) + 
  geom_point() + 
  xlim(-3.5,4) + 
  geom_line(tibble(x_vec = 2+2*dnorm(seq(-4,4,0.01), mean=1.801949, sd = 0.429405) , y_vec = seq(-4,4,0.01)), 
            mapping = aes(x = x_vec, y = y_vec), size = 1 , col = "gold", alpha = 0.4) + 
  geom_line(tibble(x_vec = -2+2*dnorm(seq(-4,4,0.01), mean=-1.801949, sd = 0.429405) , y_vec = seq(-4,4,0.01)), 
            mapping = aes(x = x_vec, y = y_vec), size = 1 , col = "gold", alpha = 0.4) + 
  theme_minimal()
```

このように$\theta$の値が固定されたとき、言い換えれば$\theta$の値が条件付けられたときに、$y$が従う確率分布のことを条件付き確率分布と呼ぶのでした。  
条件付き確率分布の密度関数のことを条件付き確率密度関数と表現し、この例では$\theta$が条件付けられたときの$y$の分布を考えているので、$p(y|\theta)$と書きます。逆に、$y$が条件付けられたときの$\theta$の確率密度関数のことを、$p(\theta|y)$と書きます。  

条件付き密度関数の数学的な定義は、以下の通りでした。  
$$
\begin{equation*}
\begin{split}
p(y|\theta) &= \frac{p(y,\theta)}{p(\theta)} \\
p(\theta | y) &= \frac{p(y,\theta)}{p(y)}
\end{split}
\end{equation*}
$$

また、上の式を変形すると、以下が成立することがわかります。  
$$
\begin{equation*}
p(y, \theta) = p(y| \theta)p(\theta) = p(\theta | y)p(y)
\end{equation*}
$$

さらに、$p(y| \theta)p(\theta) = p(\theta | y)p(y)$を変形すると以下の式を得ます。  
$$
\begin{equation*}
p(\theta | y) = \frac{p(y| \theta)p(\theta)}{p(y)}
\end{equation*}
$$

この式のことを**ベイズの定理**と呼びます。

さて、少し慣れないかもしれませんが、**$\theta$ が条件付けられたとき、$y$が確率分布$p(y|\theta)$に従っている**ということを
$$
y | \theta \sim p(y | \theta)
$$

と表現します。  

### 大事なこと1：同時分布$p(y,\theta)$を決めれば、周辺分布・条件付き分布もすべて決まる  
同時分布$p(y,\theta)$を具体的に決めたとします。(例えば先ほどのような多変量正規分布をイメージしてください)  
さて、**同時分布を決めると、周辺分布も決まります**。なぜなら、周辺分布の定義は
$$
\begin{equation*}
\begin{split}
p(y) &= \int p(y,\theta) d\theta \\
p(\theta) &= \int p(y,\theta) dy
\end{split}
\end{equation*}
$$

でしたから、同時分布を積分すれば求まるためです。同時分布を決めれば、間接的に周辺分布も決めたことになります。  
**また、条件付き分布も決まります**。先ほど確認したように、条件付き密度の定義は以下の通りでした。  
$$
\begin{equation*}
\begin{split}
p(y|\theta) &= \frac{p(y,\theta)}{p(\theta)} \\
p(\theta | y) &= \frac{p(y,\theta)}{p(y)}
\end{split}
\end{equation*}
$$

右辺の同時分布$p(y,\theta)$はすでに決まっていますし、周辺分布$p(y),p(\theta)$もまた、同時分布を決めたことで間接的に決まっているからです。  

### 大事なこと2：周辺分布$p(\theta)$と条件付き分布$p(y|\theta)$を決めれば、同時分布$p(y,\theta)$が決まる  
周辺分布$p(y),p(\theta)$を決めただけでは、同時分布$p(y,\theta)$は決まりません。  
また、条件付き分布$p(y|\theta),p(\theta |y)$を決めただけでは、同時分布$p(y,\theta)$は決まりません。  
**周辺分布$p(\theta)$と条件付き分布$p(y|\theta)$の両方を決めれば、同時分布$p(y,\theta)$が決まります**。  
(あるいは逆のパターンで、$p(y)$と$p(\theta|y)$を決めても$p(y,\theta)$は決まります)  
このことは、条件付き密度の定義を確認して、少し式変形すればすぐに分かります。  
$$
\begin{equation*}
\begin{split}
p(y|\theta) &= \frac{p(y,\theta)}{p(\theta)} \\
\Rightarrow p(y|\theta )p(\theta) &= p(y,\theta)
\end{split}
\end{equation*}
$$

さて、大事なことなので先にネタバレをしておきますが、**ベイズ統計学では、周辺分布$p(\theta)$と条件付き分布$p(y|\theta)$を分析者が決めます**。そうすると、同時分布$p(y,\theta)$が決まります。同時分布が決まったので、周辺分布$p(y)$と条件付き分布$p(\theta|y)$が決まります。  

【ベイズ統計学におけるモデリング】  
1. 分析者が$p(\theta),p(y|\theta)$を決める。  
2. ステップ.1により、自動的に同時分布$p(y,\theta)$が決まる。  
3. ステップ.2により、自動的に周辺分布$p(y)$と条件付き分布$p(\theta | y)$が決まる。  

ここで登場した条件付き確率分布$p(\theta | y)$は、ベイズ統計学では「事後分布」と呼ばれる非常に重要な分布なのですが、このあたりの話は次の章で行います。  

## 2. ベイズ統計学の基本的な考え方・基本用語  
この章ではベイズ統計学の基本的な考え方と、基本的な用語を導入します。  

### ベイズ統計学におけるパラメータ  
入門・基礎レベルで習う「通常の統計学」――これは「頻度論」と呼ばれます――では、パラメータは未知の**定数**だと考えます。  
例えば、ある小学校の6年生男子全体に対して、その母平均$\theta$を推定する問題を考えてみます。$\theta$は分析者が知ることができない何かしらの定数であって、これを推定するために手元のデータから**推定量**という値を構成・計算するのでした。（例えば標本平均は母平均を知るための推定量です）  
頻度論ではパラメータ$\theta$は定数なので、例えば$\theta$の95\%信頼区間として[131cm,162cm]が得られたとしても、「身長の平均が131cm以上、162cm以下である確率は95\%である」とは言えないのでした。パラメータ$\theta$は定数なので、$131 \leq \theta \leq 162$である確率は1か0かしかないからです。  
**……わかりにくい！！！！**  
そうです。頻度論における「不確実性評価」は言ってる意味が分かりにくいのです。  
ベイズ統計学ではこのような面倒な表現は出てきません。「身長の平均が131cm以上、162cm以下である確率は95\%である」というような表現も許されます。なぜなら、**ベイズ統計学ではパラメータを確率変数だと考えるから**です。  
頻度論とベイズ統計学って結局何が違うの？　と聞かれたら桝田はこう答えることにしています。「**パラメータが確率変数だったらそれはベイズ統計学**です」と。  
哲学的な背景は完全に異なりますが、技術的な面だけ考えるのであれば、パラメータが確率変数であるか否かが「頻度論」と「ベイズ統計学」の唯一の違いと言ってしまっても良いほどだと考えています。  

### モデリング：事前分布・尤度  
さて、ベイズ統計学ではパラメータ$\theta$は確率変数なのでした。確率変数なので、$\theta$が従う確率分布が存在します。  
この、**$\theta$が従う（周辺）確率分布のことを事前分布と呼びます**。数式で書くなら以下の通りです。  
$$
\theta \sim p(\theta)  
$$

事前分布$p(\theta)$は分析者が事前知識をもとにして決めます。  
また、**パラメータ$\theta$が条件付けられたとき（＝固定されたとき）に、データ$y$が従っている分布**のことを、**尤度**と呼びます。この表現は頻度論でも同様でした。  

【尤度の例】--------------------  
例えば、データ$y_1,y_2,\cdots,y_n$が確率分布$N(\theta, 1)$に独立に従っているとき、このことを以下のように表現しました。  
$$
y_1,y_2,\cdots,y_n | \theta \overset{i.i.d.}{\sim} N(\theta, 1)
$$

これらデータ$y_1,\cdots,y_n$をまとめて$y$と書いてしまうことにします。$(y=(y_1,\cdots,y_n))$  
さて、$y_1,\cdots,y_n$は独立なので、尤度関数はそれぞれの確率密度関数$p(y_i | \theta)\ \ (i=1,\cdots,n)$の積であり、  
$$
y|\theta \sim p(y | \theta) = p(y_1,\cdots,y_n | \theta) = \prod _{i=1}^n p(y_i | \theta) = \prod _{i=1}^n \frac{1}{\sqrt{2\pi}}\exp \left(-\frac{1}{2} (y_i - \theta)^2 \right)
$$

となります。  
【尤度の例　終わり】--------------------  

尤度もまた、事前分布と同様に分析者が決めます。  
このように、**ベイズ統計学においては、事前分布$p(\theta)$と尤度$p(y|\theta)$を分析者が決めます**。  
**このことを「モデリング」と呼びます**。  

【モデリングの例：単回帰】--------------------  
例えば、身長のデータ$y_1,y_2,\cdots,y_n$に対して、説明変数として体重のデータ$x_1,x_2,\cdots,x_n$を入れた回帰分析を考えます。  
よくある頻度論的な単回帰のモデルでは、以下のように尤度のモデリングを行っていました。  
$$
y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \ \ \epsilon_i \sim N(0,\sigma ^2)
$$

ここで、パラメータは$\beta_0,\beta_1,\sigma^2$の3つとなります。まとめて$\theta = (\beta_0,\beta_1,\sigma^2)$と書くことにしましょう。  
この式は言い換えれば、以下のことを言っているに等しいと分かります。  
$$
y_i | \theta \sim N(\beta_0 + \beta_1 x_i , \sigma^2)  
$$

尤度関数はどうなるかというと、以下のようになるのでした。($y = (y_1,y_2,\cdots,y_n)$とまとめてしまいます)  
$$
y|\theta \sim p(y|\theta) = p(y_1,\cdots,y_n | \theta) = \prod _{i = 1}^n p(y_i | \theta)
$$

ただし、$p(y_i | \theta)$は$N(\beta_0 + \beta_1 x_i , \sigma^2)$の密度関数とします。  

さて、ベイズ統計学においてはパラメータは確率変数でしたから、$\theta$の確率分布$p(\theta)=p(\beta_0,\beta_1,\sigma^2)$を決める必要があります。  
とりあえず今回は、$\beta_0, \beta_1, \sigma^2$は全て独立というモデリングにしてみます。  
独立な確率変数なので、同時確率分布$p(\beta_0,\beta_1,\sigma^2)$は周辺確率分布の積になります。  
$$
p(\theta) = p(\beta_0, \beta_1, \sigma^2) = p(\beta_0) p(\beta_1) p(\sigma^2)
$$

それぞれのパラメータの周辺分布は、例えば以下のようにしておきます。  
$$
\begin{equation*}
\begin{split}
\beta_0 &\sim N( \mu_0, \tau_0 ^2) \\
\beta_1 &\sim N( \mu_1, \tau_1 ^2) \\
\sigma ^2 &\sim Ga( a, b)
\end{split}
\end{equation*}
$$

さて、**事前分布の中にもパラメータが出てきました**。例えば$\mu_0$とかです。  
こういった事前分布の中に出てくるパラメータに対して、これもまた確率変数として扱うパターン（$\mu_0 \sim p(\mu_0)$）もあれば、ただ単に定数として扱うパターンもあります。後者の例のように、定数としてモデルの中に出てくるパラメータのことを**ハイパーパラメータ**と呼びます。  
ハイパーパラメータはデータから推定する（**まるで頻度論のごとく！**）パターンもあれば、分析者が決め打ちするパターンもあります。前者を「経験ベイズ法」と呼び、後者を「フルベイズ法」と呼びます。  
<補足>  
$\mu_0$などをハイパーパラメータとして扱うのであれば、これらはただ単に定数なので、$\beta_0,\beta_1,\sigma^2$の分布は
$$
\begin{equation*}
\begin{split}
\beta_0 &\sim N( \mu_0, \tau_0 ^2) \\
\beta_1 &\sim N( \mu_1, \tau_1 ^2) \\
\sigma ^2 &\sim Ga( a, b)
\end{split}
\end{equation*}
$$

と書きます（先ほどと同様ですね）。  
しかし、$\mu_0$などを確率変数として扱うパターンであればこの書き方はまずくて、正確には以下のように書きます。  
$$
\begin{equation*}
\begin{split}
\beta_0 | \mu_0, \tau_0 ^2 &\sim N( \mu_0, \tau_0 ^2) \\
\beta_1 | \mu_1, \tau_1 ^2 &\sim N( \mu_1, \tau_1 ^2) \\
\sigma ^2 | a,b\ \ \ \  &\sim Ga( a, b)
\end{split}
\end{equation*}
$$

【モデリングの例：単回帰　終わり】-------------------- 

### ベイズ推定：事後分布  
頻度論における「推定」とは、未知の定数であるパラメータ$\theta$を当てるための値として、推定量と呼ばれる値$\hat{\theta}$をデータから計算することを言うのでした。  
**ベイズ統計学における「推定」とは、条件付き分布$p(\theta | y)$を求めることを指します**。この確率分布$p(\theta | y)$のことを**事後分布**と呼びます。  
「事後分布$p(\theta | y)$を求める」ことを「ベイズ推定する」あるいは単に「推定する」と表現します。  
とにかく覚えておくべきは、**ベイズ統計では、事後分布を求めればOK**ということです。ベイズ統計学における推論は全て事後分布を求めるところからスタートしますし、事後分布さえ手に入ればそのほか数学的に難しいポイントは存在しません。  
とにかく覚えておくべきは、**ベイズ統計では、事後分布を求めればOK**ということです！！！  

……なぜ事後分布を求めることが推定すること（＝母集団の特性を推測すること）になるのでしょうか？　少し考えてみます。  
まず、**$\theta$の周辺分布は、事前分布$p(\theta)$でした。これは分析者が決める確率分布であり、この中には事前知識は入っていても、データの情報は入っていません**。  
(※ただし、経験ベイズ法を利用する場合はデータの情報が入ります)  
事前分布$p(\theta)$にはデータ$y$の情報が入っていないわけですが、尤度関数$p(y|\theta)$には入っています。  
さて、事後分布$p(\theta|y)$はベイズの定理から、
$$
p(\theta | y) = \frac{p(y|\theta)p(\theta)}{p(y)}
$$

でした。ここで、$y$は条件付けられている、言い換えれば$y$はただの定数なので、$p(y)$も定数となります。したがって$p(y)$での割り算というのはただ単に定数での割り算を意味しており、「どの$\theta$の値が出やすいのか」に対して全く影響を与えません。なので、$p(y)$を無視してしまいましょう。  
$$
p(\theta | y) = p(y | \theta) p(\theta) \ \ \text{（本当はイコールではない！）}
$$

「**ざっくり言えば、事後分布＝尤度×事前分布**」になっており、  
**「事後分布」＝「データの情報」×「事前知識」** の形になっていることが分かります。  
$\theta$に対する知識を、事前分布$p(\theta)$という形で既に持っている状態だとして、そこにデータの情報$p(y|\theta)$を組み込んで$\theta$に対する知識をアップデートしたものが$p(\theta|y)$となるわけです。  

イメージをつかむために、具体例を考えてみます。  
【例】-------------------  
手元に、データ$y = (y_1,y_2,y_3)$があるとします。母平均$\theta$を推測してみたいとしましょう。  
<モデリング>  
1. 尤度：$y_i | \theta \sim N(\theta , 1)$とします。  
2. 事前分布：$\theta \sim N(0,5)$とします。桝田のもつ事前知識より多分$\theta=0$くらいだと予想していますが、自信がないので分散を5と広めに取りました。  

事前分布の関数を描いてみましょう。  
```{r}
curve(dnorm(x, sd=sqrt(5)) , from = -10 , to = 10 , ylab = "p(theta)", xlab = "theta", main = "事前分布の密度関数")
```

平均値は0ですが、$\theta=3$などもありえそうな感じの分布になっています。

さて、データ$y$は具体的には以下のようだったとしましょう。  
```{r}
set.seed(200)
y = rnorm(n = 3 , mean = 5)
y
```

うわ、なんか5ぐらい行ってますね。全然0じゃなさそうだ。  
さて、尤度関数を描いてみましょう。まず数式を使って整理してみます。  
$$
p(y|\theta) = p(y_1,y_2,y_3|\theta) = \prod_{i=1}^{3} p(y_i | \theta) = \prod_{i=1}^{3} \frac{1}{\sqrt{2\pi}} \exp \left( -\frac{1}{2} (y_i - \theta)^2 \right)
$$

Rで描いてみます。**横軸に$\theta$を取ります(尤度関数は「$\theta$の関数」でした！)**。  
```{r}
Likelihood <- function(theta){
  res <- 1
  for(i in 1:3){
    res <- res * 
      dnorm(y[i] , mean = theta , sd = 1)
  }
  return(res)
}
curve(Likelihood(x) , from = -10 , to = 10 , ylab = "p(y|theta)", xlab = "theta" , main = "尤度関数")
```

**$\theta = 5$周辺に値をとる関数である**ことが分かります。  
この二つを掛け算した関数は、描画すると以下のようになります。  
```{r}
curve(Likelihood(x)*dnorm(x, sd=sqrt(5)) , from = -10 , to = 10 , ylab = "p(y|theta)×p(theta)", xlab = "theta" , main = "事後分布")
```

だいたい、尤度関数と同じ形だということがわかりました。  
このように、事前分布よりも尤度の方が「強い」場合、事後分布はほとんど尤度関数と同じ形になります。  
事前分布$p(\theta)$よりも、さらに妥当な$\theta$の確率分布$p(\theta | y)$を得ることができました！  
どうやらパラメータ$\theta$は、$\theta=5$の周辺に値を取りそうです！  
【例　終わり】-------------------  

### 予測分布  




